This project provides an infrastructure-as-code solution for deploying a full stack application with a rubust monitoring suit to Amazon EKS. The setup includes metric collection and alerting with Pormetheus and log scraping, aggregation and alterting with Promtail and Loki. Grafana is used to visualize this data, and OpenAI is intergated into the alerting pipeline to provide rich alaysis. 

ğŸ› ï¸ Features

    ğŸ§  Automated the deployment of a robust monitoring suit to AWS EKS with a single ansible script. 

    ğŸ“œ Scrapes logs from deployed application and alerts when an "ERROR" appears. 

    ğŸ“Š Uses Kube-Prometheus-Stack for standard alerts, dashboads and metric scraping. 

    ğŸ“ˆ Ability to autoscale nodes as well a pods. 

    ğŸŒ Features an Ingress controller and loadbalancer for public internet access, and proper routing even at scale. 

    ğŸ¤– Intergrates OpenIA into the alerting pipeline for rich analysis 

    ğŸ“¨ Sends enriched alterts via AWS SNS. 

    ğŸ’» Ingress configured for client side routing. 

    ğŸ’¥ Teardown script enables easy removable of the application and all dependencies, and AWS infrastructure in a single ansible script. 

ğŸ§± Architecture

    Default Namespace 

        Webapp and registry pods, the full stack application. 

    Monitoring Namespace 

        Loki deployed in a monolithic architecture. 

        Promtail scraper deployed on the same node as the full stack application collecting container logs. 

        Prometheus stack, including pods for scraping metrics, alert management, and gateway, as well as app componenets. 

        Alert analyzer, the pod dediceted to recieving alerts from prometheus alert manager, enriching them through OpenAI and then forwarding that to AWS SNS. 

    System namespace 

        Node Autoscaler provided with IAM permissions to perform the tasks of scaling up and down node in accordance to their need. (Minimum : 0, Desired : 1, Maximum : 3)

ğŸš€ Deployment

    Ensure you have the doggy EKS stack deployed from here https://github.com/DoggyApp/CloudFormationArchitecture.git to region us-east-1. Also ensure that you have your AWS profile properly configured on your personal computer, meaning the access key and secret key of the user that deployed the cluster, and with proper permissions for SSM and all needed tasks. 

        # go to your EC2 page in AWS and find the mgmt instance you just deployed, make sure it is running and then note the instance ID. (You might want to wait 20 min, the start up script installs Python10, which is needed to run the latest version of ansible, this takes a long while, so even if you log into the session once the instance appears it will still take a good while longer to spin up the environment. A good way to check that it is done is to wait for an email from AWS SNS with a confirmation to subscribe to "doggy-alerts" this is after the heavy lifting in the script). 
        # from your terminal run, 

        aws ssm start-session --target <your-instance-id>

        # this will start a remote session in that EC2 instance. 

        # you can also SSM directly into the instance from the AWS console, the above is just my prefered method, and it will be helpful later on to have done it this way if you want to view the full functionality of the environment. 

        # in the start up script for this instance we set the .bashrc file to acces the py-10 virtual environment log in with 

        sudo su - ec2-user 
        
        # and  make sure that this is your tag in the command line is "(py10-ansible-env) [ec2-user@ip-10-98-25-84 ~]$" 
        # we also git cloned this very project into the instance under /home/ec-user, make sure that it is there

        cd kubernetesArchitecture 

        # The start up script should have automatically deployed the environment, 
        # confirm that the project is running 

        kubectl get pods 

        # you should see "registry" and "webapp". 

        kubectl get pods -n monitoring 

        # you should see the "prometheus" "grafana" "loki" and "alert-analyzer". 

        kubectl get pods -n kube-system 

        # you should see "cluster-autoscaler" 

        # The below command will give you the URL to the web page so you can confirm that it is there and that the notification systems are running 

        kubectl get ingress 

        # copy the URL into your browser, you should see 2 buttons "test" and "trigger backend error" you should recieve a response that says "working" when you click on "test" this means that the frontend is able to call the backend, and you should recieve an email enriched with AI analysis when you click on "trigger backend error". (assuming that you have confimed the subscirption to doggy-alerts!)

        # you can also test that the clint side routing is working by typing in the browser "http://amazon-url/dev-server" dev-server being a common url that bots scraping the internet will attempt to access sensitive files, it should lead to a "/not-found" page.
        
        # Ensure that Grafana is running 
        # This is the command to get the password to log into the UI (it should be "prom-operator")

        kubectl get secret -n monitoring kube-prometheus-stack-grafana -o jsonpath="{.data.admin-password}" | base64 --decode

        # Grafana is running in the cluster so you have to port forward to reach it from the bastion 

        kubectl port-forward -n monitoring svc/kube-prometheus-stack-grafana 3000:80

        # the bastion doesn't have a browser and Grafana is viewed in the browser so you have to port forward again from the bastion to your local computer, you can do this with Session Manager. Open a new terminal window on your local computer and enter this command. 

        aws ssm start-session --target <your-instance-id> --document-name AWS-StartPortForwardingSession --parameters '{"portNumber":["3000"],"localPortNumber":["3000"]}'

        # go to your browser and type in http://localhost:3000 grafana will appear there, log in with the default user name "admin" and the password that you collected 3 commands ago. 

        # Once logged in you can explore and see dashboards, alert rules, logs and data sources. 

        # When you're ready to take down the application you can delete it by using the alias command that should have been set in the bashrc. file... 

        teardown 

        # go to CloudFormation in the AWS console and delete the doggy-stack from the base stack. And it will clear all traces of the application from your AWS, inclusing subsciptions to SNS topics. 

ğŸ› ï¸ Troubleshooting 

    In the bashrc. is a functions to access the ansible setup script for the appplication. The command is... "setup". If the application is not running you can run this script indepotently and read the error logs. I have attached the -vvv flag which makes them verbose. 

    also, if you cat \vars\log\user-data.log you will find a file with the output from the start up script. This will likely direct you to the problem. 

ğŸ›¡ï¸ Security 

    ğŸ” NGINX Ingress Controller has the shell in onLogin. 

    ğŸ” NGINX Ingress Controller has TLS configured to terminate in the controller. (though traffic inside the cluster is unencrypted). 

    ğŸ” NGINX files and directories are owned by root. 

    ğŸ” Ingress config has rate limits to deter scanners. 

    ğŸ” Client side routing configured to returned 404 not-found for bad urls such as "development-server".  
    
    ğŸ” Grafan is not exposed by the ingress controller, and is only accessible through port forwarding with an SSM session, or from within the cluster. 

    ğŸ” Images used on Angular and Springboot pods are the recommended. They're minmal to prevent suface area for exploitation, and set to latest for the most patched image. 

    âš ï¸ WARNING: There need to be restrictions placed on the the Kubernetes DNS so that API calls across namespaces cannot be made from the publicly accessible pods. 



